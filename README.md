# Replicating Vision Transformer (ViT) Model

This repository contains a Jupyter Notebook that replicates the Vision Transformer (ViT) model, a state-of-the-art architecture for computer vision tasks. The notebook includes data handling, model implementation, and training scripts, alongside explanations for each step.

## Overview
The Vision Transformer (ViT) applies Transformer architectures, originally designed for NLP, to image recognition tasks. This notebook demonstrates the following:
- Tokenization of images into patches.
- Implementation of Transformer-based architecture for vision tasks.
- Training and evaluation pipelines.

## Structure
- **Imports**: Essential libraries and modules are imported.
- **Setup**: Configures the device and initializes directories.
- **Model Implementation**: Defines the Vision Transformer architecture.
- **Training and Evaluation**: Scripts to train the model and validate its performance.
- **Visualizations**: Tools for analyzing the output and intermediate results.

## Acknowledgements
This work is inspired by the original Vision Transformer paper and its implementation details. Further references and resources are cited within the notebook.
- Follow up by Dipanshu following https://www.learnpytorch.io/08_pytorch_paper_replicating/
- https://github.com/mrdbourke/pytorch-deep-learning/